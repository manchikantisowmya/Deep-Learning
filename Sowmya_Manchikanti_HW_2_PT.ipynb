{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c5d3d2-2eef-4946-b394-145badcc437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c7b73-cd7a-412e-90a8-937e2480ae28",
   "metadata": {},
   "source": [
    "## Load QMNIST dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75fdc04-ccbb-4815-a7d5-e1818c1e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.QMNIST(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8424017-ed40-4140-97e3-da01931c2235",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0128510b-a20a-4b70-be6b-03a5cd17e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d016f95-ad97-4799-9dd4-706814c1ef2c",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the train and test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5416be79-6919-40c6-b61c-2d988eba91b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b650ad-9595-4af1-878f-a79c00960db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(test_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2cd30-de9c-485f-be60-49aafbd01df1",
   "metadata": {},
   "source": [
    "## Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ca99e7-58ac-40a1-a310-526961ea40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "model1 = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e58d22-35f7-4c83-8ad9-7c40856a8ec1",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ab0477-04db-4853-b5d5-f305dc015d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,train_loader, criterion, optimizer, convert=False):\n",
    "\n",
    "    # Train the neural network\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            if convert==True:\n",
    "                labels = labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7587719-32b5-412d-b97e-ff79dd903724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.061816994547844\n",
      "Epoch 1, Batch 200, Loss: 0.4360473258793354\n",
      "Epoch 1, Batch 300, Loss: 0.40424714103341103\n",
      "Epoch 1, Batch 400, Loss: 0.38280183404684065\n",
      "Epoch 1, Batch 500, Loss: 0.30820600330829623\n",
      "Epoch 1, Batch 600, Loss: 0.2991146632283926\n",
      "Epoch 1, Batch 700, Loss: 0.3061607118695974\n",
      "Epoch 1, Batch 800, Loss: 0.26715439192950724\n",
      "Epoch 1, Batch 900, Loss: 0.2737673832476139\n",
      "Epoch 2, Batch 100, Loss: 0.23594975218176842\n",
      "Epoch 2, Batch 200, Loss: 0.22049896776676178\n",
      "Epoch 2, Batch 300, Loss: 0.1970886568725109\n",
      "Epoch 2, Batch 400, Loss: 0.19497104302048684\n",
      "Epoch 2, Batch 500, Loss: 0.1936086367815733\n",
      "Epoch 2, Batch 600, Loss: 0.17917533215135337\n",
      "Epoch 2, Batch 700, Loss: 0.18168642316013575\n",
      "Epoch 2, Batch 800, Loss: 0.16888135217130185\n",
      "Epoch 2, Batch 900, Loss: 0.17630338279530405\n",
      "Epoch 3, Batch 100, Loss: 0.1398440705612302\n",
      "Epoch 3, Batch 200, Loss: 0.14289604788646101\n",
      "Epoch 3, Batch 300, Loss: 0.1478538844548166\n",
      "Epoch 3, Batch 400, Loss: 0.14022552311420441\n",
      "Epoch 3, Batch 500, Loss: 0.13046027628704907\n",
      "Epoch 3, Batch 600, Loss: 0.13715088656172156\n",
      "Epoch 3, Batch 700, Loss: 0.12484456486999988\n",
      "Epoch 3, Batch 800, Loss: 0.13372418520972132\n",
      "Epoch 3, Batch 900, Loss: 0.12893713016062974\n",
      "Epoch 4, Batch 100, Loss: 0.09939761163666844\n",
      "Epoch 4, Batch 200, Loss: 0.11055366806685925\n",
      "Epoch 4, Batch 300, Loss: 0.11217172375880181\n",
      "Epoch 4, Batch 400, Loss: 0.10878526899032295\n",
      "Epoch 4, Batch 500, Loss: 0.10899097004905343\n",
      "Epoch 4, Batch 600, Loss: 0.11438941886648536\n",
      "Epoch 4, Batch 700, Loss: 0.11715915871784091\n",
      "Epoch 4, Batch 800, Loss: 0.10976098952814936\n",
      "Epoch 4, Batch 900, Loss: 0.10976956184953451\n",
      "Epoch 5, Batch 100, Loss: 0.10314281087368726\n",
      "Epoch 5, Batch 200, Loss: 0.0991766244545579\n",
      "Epoch 5, Batch 300, Loss: 0.09097057642415166\n",
      "Epoch 5, Batch 400, Loss: 0.09264111894182861\n",
      "Epoch 5, Batch 500, Loss: 0.07863762689754367\n",
      "Epoch 5, Batch 600, Loss: 0.09387198335956783\n",
      "Epoch 5, Batch 700, Loss: 0.09253316552378238\n",
      "Epoch 5, Batch 800, Loss: 0.10599025772418827\n",
      "Epoch 5, Batch 900, Loss: 0.08638183787930757\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "training(model1,train_loader,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d7467-eadd-40ef-b954-d7203cf422a9",
   "metadata": {},
   "source": [
    "## Evaluate the model with training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b83a9b9-a084-4ca9-a550-555a274493e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModelwithTrain(model,train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    result = 100 * (correct / total)\n",
    "    print(f'Accuracy with training dataset: {result}%')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a462c-da91-4a6e-b637-e9f79be05755",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset and store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d010adf2-f230-4f90-b3eb-800c0e2b00ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "flabels = []\n",
    "fimages = []\n",
    "def evalModelwithTest(model,train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    global predictions, flabels, fimages\n",
    "    predictions = []\n",
    "    flabels = []\n",
    "    fimages = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            timages, tlabels = data\n",
    "            outputs = model(timages)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predNumpy = predicted.numpy();\n",
    "            predictions.extend(predNumpy)  # Store predictions\n",
    "            flabels.extend(tlabels.numpy())\n",
    "            fimages.extend(timages)\n",
    "            total += tlabels.size(0)\n",
    "            correct += (predicted == tlabels).sum().item()\n",
    "    result = 100 * (correct / total)\n",
    "    print(f'Accuracy with test dataset: {result}%')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229686e-611d-47f6-9b06-2072e9b32c10",
   "metadata": {},
   "source": [
    "# Step 3. Report on the results in terms of prediction accuracy on the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bc047e-045b-4d00-8454-000b28a26835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training dataset: 97.605%\n",
      "Accuracy with test dataset: 96.42666666666668%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.42666666666668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalModelwithTrain(model1,train_loader)\n",
    "evalModelwithTest(model1,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c040be-b81a-4e05-a987-8479aab91bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get one test image and its label\n",
    "\n",
    "testIndex = 4\n",
    "\n",
    "timage, tlabel = fimages[testIndex], flabels[testIndex]\n",
    "print(timage.shape)\n",
    "# Reshape the image tensor to a 28x28 shape\n",
    "timage = timage.view(28, 28)\n",
    "\n",
    "# Convert the image tensor to a numpy array for visualization\n",
    "image_numpy = timage.numpy()\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(image_numpy, cmap='gray')\n",
    "plt.title(f'Predicted Label: {predictions[testIndex].item()}, Actual Label: {tlabel.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd8057-2864-484a-9791-0a25e3b779b9",
   "metadata": {},
   "source": [
    "# Step 4. Increase the current number of nodes in the layer to 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd73a8-b671-44d0-9ba9-d3799958ea4b",
   "metadata": {},
   "source": [
    "### Hypothesis: Performance of the model increases with increase in number of nodes in the layer to 256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b35c0-d683-4ca5-88f4-0c9bfb7b7159",
   "metadata": {},
   "source": [
    "# Step 5. Modify the model based on the chosen method and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa01a42e-c99e-4cbf-9cab-fafa90015ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "model2 = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42560bc-93a7-41dc-9564-00787a3d3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.8985725602507592\n",
      "Epoch 1, Batch 200, Loss: 0.40871361643075943\n",
      "Epoch 1, Batch 300, Loss: 0.33459182471036913\n",
      "Epoch 1, Batch 400, Loss: 0.3114462473988533\n",
      "Epoch 1, Batch 500, Loss: 0.2774693172425032\n",
      "Epoch 1, Batch 600, Loss: 0.22221440993249417\n",
      "Epoch 1, Batch 700, Loss: 0.22256406173110008\n",
      "Epoch 1, Batch 800, Loss: 0.23086966313421725\n",
      "Epoch 1, Batch 900, Loss: 0.19748420249670745\n",
      "Epoch 2, Batch 100, Loss: 0.17127276256680488\n",
      "Epoch 2, Batch 200, Loss: 0.15057542975991964\n",
      "Epoch 2, Batch 300, Loss: 0.17812605516985058\n",
      "Epoch 2, Batch 400, Loss: 0.1503095829859376\n",
      "Epoch 2, Batch 500, Loss: 0.14715852899476886\n",
      "Epoch 2, Batch 600, Loss: 0.15627979036420583\n",
      "Epoch 2, Batch 700, Loss: 0.15206062028184533\n",
      "Epoch 2, Batch 800, Loss: 0.14678342172876\n",
      "Epoch 2, Batch 900, Loss: 0.13088338812813163\n",
      "Epoch 3, Batch 100, Loss: 0.10776589822024107\n",
      "Epoch 3, Batch 200, Loss: 0.11611794227734208\n",
      "Epoch 3, Batch 300, Loss: 0.11744940891861916\n",
      "Epoch 3, Batch 400, Loss: 0.11186094479635358\n",
      "Epoch 3, Batch 500, Loss: 0.12848459049127997\n",
      "Epoch 3, Batch 600, Loss: 0.1144270044658333\n",
      "Epoch 3, Batch 700, Loss: 0.11301636308431626\n",
      "Epoch 3, Batch 800, Loss: 0.10284266262315214\n",
      "Epoch 3, Batch 900, Loss: 0.11162407422438264\n",
      "Epoch 4, Batch 100, Loss: 0.09068117933347822\n",
      "Epoch 4, Batch 200, Loss: 0.08606613705866038\n",
      "Epoch 4, Batch 300, Loss: 0.08712533437646926\n",
      "Epoch 4, Batch 400, Loss: 0.09001455888152123\n",
      "Epoch 4, Batch 500, Loss: 0.09920718563254922\n",
      "Epoch 4, Batch 600, Loss: 0.09607410577125847\n",
      "Epoch 4, Batch 700, Loss: 0.09444078178144992\n",
      "Epoch 4, Batch 800, Loss: 0.09616991110146046\n",
      "Epoch 4, Batch 900, Loss: 0.0975600461754948\n",
      "Epoch 5, Batch 100, Loss: 0.0728985251346603\n",
      "Epoch 5, Batch 200, Loss: 0.08530042994767427\n",
      "Epoch 5, Batch 300, Loss: 0.07226381886284798\n",
      "Epoch 5, Batch 400, Loss: 0.07621476467698812\n",
      "Epoch 5, Batch 500, Loss: 0.07516135605052114\n",
      "Epoch 5, Batch 600, Loss: 0.07055722943972796\n",
      "Epoch 5, Batch 700, Loss: 0.07714113920461386\n",
      "Epoch 5, Batch 800, Loss: 0.07887021080590785\n",
      "Epoch 5, Batch 900, Loss: 0.07626439499668777\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "training(model2,train_loader,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7faaef0-83fd-4cbb-8f0b-defb31ab1e59",
   "metadata": {},
   "source": [
    "# Step 6. Report the Results of the modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eff6a53-16bd-408b-8424-d1945600756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training dataset: 97.38333333333333%\n",
      "Accuracy with test dataset: 96.17166666666667%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.17166666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalModelwithTrain(model2,train_loader)\n",
    "evalModelwithTest(model2,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0674d088-9fc9-4f44-95ef-72d8ade40ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the model improved which is shown in both Training and Test datasets!\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of the model improved which is shown in both Training and Test datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97333ab-228c-449d-a2f6-4b89ae870fbe",
   "metadata": {},
   "source": [
    "# Step 7. Experiment with different optimizers, loss functions, dropout, and activation functions, and observe the change in performance as you tune these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a279385-273a-4820-a4d9-d93ca21c1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different optimizers, activation and  loss functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modify your model's forward function\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)  # Applied log-softmax\n",
    "        return x\n",
    "model3 = MLP();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "261e2177-7fb1-499c-bf01-ff935f83f94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 2.3002550625801086\n",
      "Epoch 1, Batch 200, Loss: 2.283585636615753\n",
      "Epoch 1, Batch 300, Loss: 2.267919034957886\n",
      "Epoch 1, Batch 400, Loss: 2.252979781627655\n",
      "Epoch 1, Batch 500, Loss: 2.2351708126068117\n",
      "Epoch 1, Batch 600, Loss: 2.213322069644928\n",
      "Epoch 1, Batch 700, Loss: 2.194905364513397\n",
      "Epoch 1, Batch 800, Loss: 2.171711735725403\n",
      "Epoch 1, Batch 900, Loss: 2.148003165721893\n",
      "Epoch 2, Batch 100, Loss: 2.112252399921417\n",
      "Epoch 2, Batch 200, Loss: 2.081825394630432\n",
      "Epoch 2, Batch 300, Loss: 2.0489038383960723\n",
      "Epoch 2, Batch 400, Loss: 2.017187865972519\n",
      "Epoch 2, Batch 500, Loss: 1.9783679044246674\n",
      "Epoch 2, Batch 600, Loss: 1.933749715089798\n",
      "Epoch 2, Batch 700, Loss: 1.8937742340564727\n",
      "Epoch 2, Batch 800, Loss: 1.8490628969669343\n",
      "Epoch 2, Batch 900, Loss: 1.795780245065689\n",
      "Epoch 3, Batch 100, Loss: 1.727423769235611\n",
      "Epoch 3, Batch 200, Loss: 1.6611605608463287\n",
      "Epoch 3, Batch 300, Loss: 1.6213914966583252\n",
      "Epoch 3, Batch 400, Loss: 1.560092604160309\n",
      "Epoch 3, Batch 500, Loss: 1.5057232546806336\n",
      "Epoch 3, Batch 600, Loss: 1.4525015676021575\n",
      "Epoch 3, Batch 700, Loss: 1.388476438522339\n",
      "Epoch 3, Batch 800, Loss: 1.342897584438324\n",
      "Epoch 3, Batch 900, Loss: 1.3054641997814178\n",
      "Epoch 4, Batch 100, Loss: 1.224145187139511\n",
      "Epoch 4, Batch 200, Loss: 1.1749007558822633\n",
      "Epoch 4, Batch 300, Loss: 1.1430175799131392\n",
      "Epoch 4, Batch 400, Loss: 1.0806222766637803\n",
      "Epoch 4, Batch 500, Loss: 1.047739046216011\n",
      "Epoch 4, Batch 600, Loss: 1.0243823558092118\n",
      "Epoch 4, Batch 700, Loss: 0.9819291055202484\n",
      "Epoch 4, Batch 800, Loss: 0.9384419816732407\n",
      "Epoch 4, Batch 900, Loss: 0.9138459712266922\n",
      "Epoch 5, Batch 100, Loss: 0.8708143019676209\n",
      "Epoch 5, Batch 200, Loss: 0.8506782019138336\n",
      "Epoch 5, Batch 300, Loss: 0.8229987341165542\n",
      "Epoch 5, Batch 400, Loss: 0.7840078943967819\n",
      "Epoch 5, Batch 500, Loss: 0.7837907439470291\n",
      "Epoch 5, Batch 600, Loss: 0.7628801101446152\n",
      "Epoch 5, Batch 700, Loss: 0.7416508066654205\n",
      "Epoch 5, Batch 800, Loss: 0.7093043065071106\n",
      "Epoch 5, Batch 900, Loss: 0.7024001526832581\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss() # Negative Log-Likelihood Loss Function \n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.001) # stochastic gradient descent with momentum\n",
    "training(model3,train_loader,criterion,optimizer,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83315d48-b3f7-4264-b75a-bf157ecfff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with training dataset: 83.83166666666668%\n",
      "Accuracy with test dataset: 83.82333333333334%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.82333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalModelwithTrain(model3,train_loader)\n",
    "evalModelwithTest(model3,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad921e1f-1eb8-45c6-8035-f93f06a7a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the model is reduced using NLL loss function applied with log-softmax, SGD optimizer\n"
     ]
    }
   ],
   "source": [
    "print(\"The performance of the model is reduced using NLL loss function applied with log-softmax, SGD optimizer\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "852248fd-b43e-44c3-9505-2c6bb4e6ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedMLP(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(TunedMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "def training(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60fb43-072c-43e5-bf53-9c03a9b8c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimenting with Optimizer: SGD, Loss: CrossEntropyLoss, Activation: relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.294035427570343\n",
      "Epoch 1, Batch 200, Loss: 2.277443838119507\n",
      "Epoch 1, Batch 300, Loss: 2.2582648015022277\n",
      "Epoch 1, Batch 400, Loss: 2.2418451571464537\n",
      "Epoch 1, Batch 500, Loss: 2.2221875262260435\n",
      "Epoch 1, Batch 600, Loss: 2.205084590911865\n",
      "Epoch 1, Batch 700, Loss: 2.1834514689445497\n",
      "Epoch 1, Batch 800, Loss: 2.1586793088912963\n",
      "Epoch 1, Batch 900, Loss: 2.1398246002197268\n",
      "Epoch 2, Batch 100, Loss: 2.1006210374832155\n",
      "Epoch 2, Batch 200, Loss: 2.0694668805599212\n",
      "Epoch 2, Batch 300, Loss: 2.040208467245102\n",
      "Epoch 2, Batch 400, Loss: 2.012355298995972\n",
      "Epoch 2, Batch 500, Loss: 1.981854375600815\n",
      "Epoch 2, Batch 600, Loss: 1.9525963973999023\n",
      "Epoch 2, Batch 700, Loss: 1.914734605550766\n",
      "Epoch 2, Batch 800, Loss: 1.8833457052707672\n",
      "Epoch 2, Batch 900, Loss: 1.8483332681655884\n",
      "Epoch 3, Batch 100, Loss: 1.785906856060028\n",
      "Epoch 3, Batch 200, Loss: 1.7573161673545838\n",
      "Epoch 3, Batch 300, Loss: 1.7182024037837982\n",
      "Epoch 3, Batch 400, Loss: 1.6694990015029907\n",
      "Epoch 3, Batch 500, Loss: 1.644817281961441\n",
      "Epoch 3, Batch 600, Loss: 1.5810250806808472\n",
      "Epoch 3, Batch 700, Loss: 1.5448910236358642\n",
      "Epoch 3, Batch 800, Loss: 1.5081852161884308\n",
      "Epoch 3, Batch 900, Loss: 1.4703501522541047\n",
      "Epoch 4, Batch 100, Loss: 1.4355232071876527\n",
      "Epoch 4, Batch 200, Loss: 1.3937315690517424\n",
      "Epoch 4, Batch 300, Loss: 1.361708961725235\n",
      "Epoch 4, Batch 400, Loss: 1.3329280626773834\n",
      "Epoch 4, Batch 500, Loss: 1.3184277868270875\n",
      "Epoch 4, Batch 600, Loss: 1.2776709127426147\n",
      "Epoch 4, Batch 700, Loss: 1.2579501950740815\n",
      "Epoch 4, Batch 800, Loss: 1.2185166311264037\n",
      "Epoch 4, Batch 900, Loss: 1.2018653047084809\n",
      "Epoch 5, Batch 100, Loss: 1.160718678832054\n",
      "Epoch 5, Batch 200, Loss: 1.1360753017663956\n",
      "Epoch 5, Batch 300, Loss: 1.133144846558571\n",
      "Epoch 5, Batch 400, Loss: 1.100903913974762\n",
      "Epoch 5, Batch 500, Loss: 1.0942076653242112\n",
      "Epoch 5, Batch 600, Loss: 1.0807908755540847\n",
      "Epoch 5, Batch 700, Loss: 1.0484323114156724\n",
      "Epoch 5, Batch 800, Loss: 1.0427450448274613\n",
      "Epoch 5, Batch 900, Loss: 1.0175518488883972\n",
      "Finished Training\n",
      "Accuracy with training dataset: 80.86833333333333%\n",
      "Accuracy with test dataset: 80.86833333333333%\n",
      "\n",
      "Experimenting with Optimizer: SGD, Loss: CrossEntropyLoss, Activation: leaky_relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.3184265995025637\n",
      "Epoch 1, Batch 200, Loss: 2.296890757083893\n",
      "Epoch 1, Batch 300, Loss: 2.278519070148468\n",
      "Epoch 1, Batch 400, Loss: 2.2622563552856447\n",
      "Epoch 1, Batch 500, Loss: 2.247193818092346\n",
      "Epoch 1, Batch 600, Loss: 2.2339633083343506\n",
      "Epoch 1, Batch 700, Loss: 2.215794246196747\n",
      "Epoch 1, Batch 800, Loss: 2.1937834358215333\n",
      "Epoch 1, Batch 900, Loss: 2.1786311888694763\n",
      "Epoch 2, Batch 100, Loss: 2.1457746982574464\n",
      "Epoch 2, Batch 200, Loss: 2.126974489688873\n",
      "Epoch 2, Batch 300, Loss: 2.106423375606537\n",
      "Epoch 2, Batch 400, Loss: 2.080642112493515\n",
      "Epoch 2, Batch 500, Loss: 2.0477342522144317\n",
      "Epoch 2, Batch 600, Loss: 2.011883158683777\n",
      "Epoch 2, Batch 700, Loss: 1.9870664656162262\n",
      "Epoch 2, Batch 800, Loss: 1.966548331975937\n",
      "Epoch 2, Batch 900, Loss: 1.929061381816864\n",
      "Epoch 3, Batch 100, Loss: 1.8664562702178955\n",
      "Epoch 3, Batch 200, Loss: 1.8406136560440063\n",
      "Epoch 3, Batch 300, Loss: 1.8033828389644624\n",
      "Epoch 3, Batch 400, Loss: 1.7539888370037078\n",
      "Epoch 3, Batch 500, Loss: 1.7283194363117218\n",
      "Epoch 3, Batch 600, Loss: 1.6821652007102967\n",
      "Epoch 3, Batch 700, Loss: 1.6581541049480437\n",
      "Epoch 3, Batch 800, Loss: 1.6119115769863128\n",
      "Epoch 3, Batch 900, Loss: 1.5677726447582245\n",
      "Epoch 4, Batch 100, Loss: 1.5157556653022766\n",
      "Epoch 4, Batch 200, Loss: 1.5078439438343048\n",
      "Epoch 4, Batch 300, Loss: 1.460564216375351\n",
      "Epoch 4, Batch 400, Loss: 1.4190008056163788\n",
      "Epoch 4, Batch 500, Loss: 1.4031833052635192\n",
      "Epoch 4, Batch 600, Loss: 1.3645505487918854\n",
      "Epoch 4, Batch 700, Loss: 1.3583254325389862\n",
      "Epoch 4, Batch 800, Loss: 1.330382010936737\n",
      "Epoch 4, Batch 900, Loss: 1.2932329511642455\n",
      "Epoch 5, Batch 100, Loss: 1.2511186069250106\n",
      "Epoch 5, Batch 200, Loss: 1.23537338078022\n",
      "Epoch 5, Batch 300, Loss: 1.2081366765499115\n",
      "Epoch 5, Batch 400, Loss: 1.1856846022605896\n",
      "Epoch 5, Batch 500, Loss: 1.1672810155153275\n",
      "Epoch 5, Batch 600, Loss: 1.1474767023324965\n",
      "Epoch 5, Batch 700, Loss: 1.143687877058983\n",
      "Epoch 5, Batch 800, Loss: 1.1080094182491302\n",
      "Epoch 5, Batch 900, Loss: 1.083855401277542\n",
      "Finished Training\n",
      "Accuracy with training dataset: 79.92333333333333%\n",
      "Accuracy with test dataset: 79.865%\n",
      "\n",
      "Experimenting with Optimizer: SGD, Loss: CrossEntropyLoss, Activation: elu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.2972621369361876\n",
      "Epoch 1, Batch 200, Loss: 2.276192843914032\n",
      "Epoch 1, Batch 300, Loss: 2.254180524349213\n",
      "Epoch 1, Batch 400, Loss: 2.237415041923523\n",
      "Epoch 1, Batch 500, Loss: 2.2132540059089663\n",
      "Epoch 1, Batch 600, Loss: 2.1913822650909425\n",
      "Epoch 1, Batch 700, Loss: 2.1687557435035707\n",
      "Epoch 1, Batch 800, Loss: 2.144399700164795\n",
      "Epoch 1, Batch 900, Loss: 2.120237228870392\n",
      "Epoch 2, Batch 100, Loss: 2.0854001808166505\n",
      "Epoch 2, Batch 200, Loss: 2.0568831956386564\n",
      "Epoch 2, Batch 300, Loss: 2.0285444498062133\n",
      "Epoch 2, Batch 400, Loss: 1.9876673185825349\n",
      "Epoch 2, Batch 500, Loss: 1.9499768340587615\n",
      "Epoch 2, Batch 600, Loss: 1.9150668239593507\n",
      "Epoch 2, Batch 700, Loss: 1.8850369322299958\n",
      "Epoch 2, Batch 800, Loss: 1.8341889691352844\n",
      "Epoch 2, Batch 900, Loss: 1.8005512380599975\n",
      "Epoch 3, Batch 100, Loss: 1.7424105441570281\n",
      "Epoch 3, Batch 200, Loss: 1.709339257478714\n",
      "Epoch 3, Batch 300, Loss: 1.658198355436325\n",
      "Epoch 3, Batch 400, Loss: 1.6235024070739745\n",
      "Epoch 3, Batch 500, Loss: 1.5898636054992676\n",
      "Epoch 3, Batch 600, Loss: 1.539656412601471\n",
      "Epoch 3, Batch 700, Loss: 1.5184809267520905\n",
      "Epoch 3, Batch 800, Loss: 1.4805235028266908\n",
      "Epoch 3, Batch 900, Loss: 1.457646597623825\n",
      "Epoch 4, Batch 100, Loss: 1.3918613255023957\n",
      "Epoch 4, Batch 200, Loss: 1.3646132707595826\n",
      "Epoch 4, Batch 300, Loss: 1.345252068042755\n",
      "Epoch 4, Batch 400, Loss: 1.3098428010940553\n",
      "Epoch 4, Batch 500, Loss: 1.2716230750083923\n",
      "Epoch 4, Batch 600, Loss: 1.2514219331741332\n",
      "Epoch 4, Batch 700, Loss: 1.239967200756073\n",
      "Epoch 4, Batch 800, Loss: 1.2181956923007966\n",
      "Epoch 4, Batch 900, Loss: 1.1908447521924972\n",
      "Epoch 5, Batch 100, Loss: 1.155484654903412\n",
      "Epoch 5, Batch 200, Loss: 1.1229680061340332\n",
      "Epoch 5, Batch 300, Loss: 1.129946926832199\n",
      "Epoch 5, Batch 400, Loss: 1.0972054785490035\n",
      "Epoch 5, Batch 500, Loss: 1.078049023747444\n",
      "Epoch 5, Batch 600, Loss: 1.074297866821289\n",
      "Epoch 5, Batch 700, Loss: 1.0340156775712968\n",
      "Epoch 5, Batch 800, Loss: 1.0236299538612366\n",
      "Epoch 5, Batch 900, Loss: 1.0118279600143432\n",
      "Finished Training\n",
      "Accuracy with training dataset: 80.175%\n",
      "Accuracy with test dataset: 80.25666666666666%\n",
      "\n",
      "Experimenting with Optimizer: SGD, Loss: NLLLoss, Activation: relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.298117277622223\n",
      "Epoch 1, Batch 200, Loss: 2.279287540912628\n",
      "Epoch 1, Batch 300, Loss: 2.2665460681915284\n",
      "Epoch 1, Batch 400, Loss: 2.257367513179779\n",
      "Epoch 1, Batch 500, Loss: 2.2402759313583376\n",
      "Epoch 1, Batch 600, Loss: 2.225394706726074\n",
      "Epoch 1, Batch 700, Loss: 2.2123917388916015\n",
      "Epoch 1, Batch 800, Loss: 2.201931631565094\n",
      "Epoch 1, Batch 900, Loss: 2.17733179807663\n",
      "Epoch 2, Batch 100, Loss: 2.152640504837036\n",
      "Epoch 2, Batch 200, Loss: 2.13430802822113\n",
      "Epoch 2, Batch 300, Loss: 2.1143644309043883\n",
      "Epoch 2, Batch 400, Loss: 2.0915689301490783\n",
      "Epoch 2, Batch 500, Loss: 2.062393615245819\n",
      "Epoch 2, Batch 600, Loss: 2.03395884513855\n",
      "Epoch 2, Batch 700, Loss: 2.0019810402393343\n",
      "Epoch 2, Batch 800, Loss: 1.9820735228061677\n",
      "Epoch 2, Batch 900, Loss: 1.9480082452297212\n",
      "Epoch 3, Batch 100, Loss: 1.8917846012115478\n",
      "Epoch 3, Batch 200, Loss: 1.8648712301254273\n",
      "Epoch 3, Batch 300, Loss: 1.8352334797382355\n",
      "Epoch 3, Batch 400, Loss: 1.7828021264076233\n",
      "Epoch 3, Batch 500, Loss: 1.754761346578598\n",
      "Epoch 3, Batch 600, Loss: 1.7246843981742859\n",
      "Epoch 3, Batch 700, Loss: 1.6913043773174286\n",
      "Epoch 3, Batch 800, Loss: 1.6597617399692535\n",
      "Epoch 3, Batch 900, Loss: 1.6211523699760437\n",
      "Epoch 4, Batch 100, Loss: 1.5615892219543457\n",
      "Epoch 4, Batch 200, Loss: 1.5321482169628142\n",
      "Epoch 4, Batch 300, Loss: 1.5006321156024933\n",
      "Epoch 4, Batch 400, Loss: 1.446521806716919\n",
      "Epoch 4, Batch 500, Loss: 1.4218525910377502\n",
      "Epoch 4, Batch 600, Loss: 1.393827714920044\n",
      "Epoch 4, Batch 700, Loss: 1.3631763172149658\n",
      "Epoch 4, Batch 800, Loss: 1.3393199265003204\n",
      "Epoch 4, Batch 900, Loss: 1.3088680243492126\n",
      "Epoch 5, Batch 100, Loss: 1.2688085532188416\n",
      "Epoch 5, Batch 200, Loss: 1.2425671046972275\n",
      "Epoch 5, Batch 300, Loss: 1.2285630482435226\n",
      "Epoch 5, Batch 400, Loss: 1.215039312839508\n",
      "Epoch 5, Batch 500, Loss: 1.1834282553195954\n",
      "Epoch 5, Batch 600, Loss: 1.1653037774562836\n",
      "Epoch 5, Batch 700, Loss: 1.1394845563173295\n",
      "Epoch 5, Batch 800, Loss: 1.1333151006698607\n",
      "Epoch 5, Batch 900, Loss: 1.1172197270393371\n",
      "Finished Training\n",
      "Accuracy with training dataset: 80.71666666666667%\n",
      "Accuracy with test dataset: 80.58166666666666%\n",
      "\n",
      "Experimenting with Optimizer: SGD, Loss: NLLLoss, Activation: leaky_relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.3119490456581118\n",
      "Epoch 1, Batch 200, Loss: 2.2944128799438475\n",
      "Epoch 1, Batch 300, Loss: 2.2817081093788145\n",
      "Epoch 1, Batch 400, Loss: 2.2690787720680237\n",
      "Epoch 1, Batch 500, Loss: 2.2557690572738647\n",
      "Epoch 1, Batch 600, Loss: 2.2456970906257627\n",
      "Epoch 1, Batch 700, Loss: 2.2286587309837342\n",
      "Epoch 1, Batch 800, Loss: 2.214836072921753\n",
      "Epoch 1, Batch 900, Loss: 2.199098644256592\n",
      "Epoch 2, Batch 100, Loss: 2.1739926099777223\n",
      "Epoch 2, Batch 200, Loss: 2.1574604678153992\n",
      "Epoch 2, Batch 300, Loss: 2.1380781722068787\n",
      "Epoch 2, Batch 400, Loss: 2.115502462387085\n",
      "Epoch 2, Batch 500, Loss: 2.090632119178772\n",
      "Epoch 2, Batch 600, Loss: 2.0661188650131224\n",
      "Epoch 2, Batch 700, Loss: 2.037217608690262\n",
      "Epoch 2, Batch 800, Loss: 2.016568043231964\n",
      "Epoch 2, Batch 900, Loss: 1.984043024778366\n",
      "Epoch 3, Batch 100, Loss: 1.9361466920375825\n",
      "Epoch 3, Batch 200, Loss: 1.9075051271915435\n",
      "Epoch 3, Batch 300, Loss: 1.8739461159706117\n",
      "Epoch 3, Batch 400, Loss: 1.8332404482364655\n",
      "Epoch 3, Batch 500, Loss: 1.8113818442821503\n",
      "Epoch 3, Batch 600, Loss: 1.7678737354278564\n",
      "Epoch 3, Batch 700, Loss: 1.7299693632125854\n",
      "Epoch 3, Batch 800, Loss: 1.6861677002906799\n",
      "Epoch 3, Batch 900, Loss: 1.6431044697761537\n",
      "Epoch 4, Batch 100, Loss: 1.6039290606975556\n",
      "Epoch 4, Batch 200, Loss: 1.5643350684642792\n",
      "Epoch 4, Batch 300, Loss: 1.5306580460071564\n",
      "Epoch 4, Batch 400, Loss: 1.489546011686325\n",
      "Epoch 4, Batch 500, Loss: 1.4427813601493835\n",
      "Epoch 4, Batch 600, Loss: 1.4234258890151978\n",
      "Epoch 4, Batch 700, Loss: 1.3931180369853973\n",
      "Epoch 4, Batch 800, Loss: 1.3474792873859405\n",
      "Epoch 4, Batch 900, Loss: 1.3233767008781434\n",
      "Epoch 5, Batch 100, Loss: 1.2866874063014984\n",
      "Epoch 5, Batch 200, Loss: 1.2694893276691437\n",
      "Epoch 5, Batch 300, Loss: 1.2370332634449006\n",
      "Epoch 5, Batch 400, Loss: 1.225366965532303\n",
      "Epoch 5, Batch 500, Loss: 1.1970349502563478\n",
      "Epoch 5, Batch 600, Loss: 1.171233469247818\n",
      "Epoch 5, Batch 700, Loss: 1.1394198673963547\n",
      "Epoch 5, Batch 800, Loss: 1.1441996949911117\n",
      "Epoch 5, Batch 900, Loss: 1.1213287723064422\n",
      "Finished Training\n",
      "Accuracy with training dataset: 79.50666666666667%\n",
      "Accuracy with test dataset: 79.655%\n",
      "\n",
      "Experimenting with Optimizer: SGD, Loss: NLLLoss, Activation: elu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 2.2909071540832517\n",
      "Epoch 1, Batch 200, Loss: 2.2696626114845277\n",
      "Epoch 1, Batch 300, Loss: 2.2550464820861817\n",
      "Epoch 1, Batch 400, Loss: 2.234362015724182\n",
      "Epoch 1, Batch 500, Loss: 2.217628974914551\n",
      "Epoch 1, Batch 600, Loss: 2.1897215676307678\n",
      "Epoch 1, Batch 700, Loss: 2.1726187014579774\n",
      "Epoch 1, Batch 800, Loss: 2.149298129081726\n",
      "Epoch 1, Batch 900, Loss: 2.1203957772254944\n",
      "Epoch 2, Batch 100, Loss: 2.081658413410187\n",
      "Epoch 2, Batch 200, Loss: 2.048262335062027\n",
      "Epoch 2, Batch 300, Loss: 2.019867689609528\n",
      "Epoch 2, Batch 400, Loss: 1.9930300664901734\n",
      "Epoch 2, Batch 500, Loss: 1.9568160963058472\n",
      "Epoch 2, Batch 600, Loss: 1.9137848734855651\n",
      "Epoch 2, Batch 700, Loss: 1.8777159810066224\n",
      "Epoch 2, Batch 800, Loss: 1.8410892951488496\n",
      "Epoch 2, Batch 900, Loss: 1.7989438545703889\n",
      "Epoch 3, Batch 100, Loss: 1.7451682603359222\n",
      "Epoch 3, Batch 200, Loss: 1.7017604684829712\n",
      "Epoch 3, Batch 300, Loss: 1.6678620278835297\n",
      "Epoch 3, Batch 400, Loss: 1.634868849515915\n",
      "Epoch 3, Batch 500, Loss: 1.5826586437225343\n",
      "Epoch 3, Batch 600, Loss: 1.5563461124897002\n",
      "Epoch 3, Batch 700, Loss: 1.5188618302345276\n",
      "Epoch 3, Batch 800, Loss: 1.4784189760684967\n",
      "Epoch 3, Batch 900, Loss: 1.4498246896266938\n",
      "Epoch 4, Batch 100, Loss: 1.4160229825973512\n",
      "Epoch 4, Batch 200, Loss: 1.3778099346160888\n",
      "Epoch 4, Batch 300, Loss: 1.3318951153755187\n",
      "Epoch 4, Batch 400, Loss: 1.2962968611717225\n",
      "Epoch 4, Batch 500, Loss: 1.2894362425804138\n",
      "Epoch 4, Batch 600, Loss: 1.2605535185337067\n",
      "Epoch 4, Batch 700, Loss: 1.2306585657596587\n",
      "Epoch 4, Batch 800, Loss: 1.2142821061611175\n",
      "Epoch 4, Batch 900, Loss: 1.2015178966522218\n",
      "Epoch 5, Batch 100, Loss: 1.1581312030553819\n",
      "Epoch 5, Batch 200, Loss: 1.131550191640854\n",
      "Epoch 5, Batch 300, Loss: 1.1148554903268815\n",
      "Epoch 5, Batch 400, Loss: 1.117953370809555\n",
      "Epoch 5, Batch 500, Loss: 1.0857494926452638\n",
      "Epoch 5, Batch 600, Loss: 1.0716534978151322\n",
      "Epoch 5, Batch 700, Loss: 1.0464133417606354\n",
      "Epoch 5, Batch 800, Loss: 1.045380912423134\n",
      "Epoch 5, Batch 900, Loss: 1.040453497171402\n",
      "Finished Training\n",
      "Accuracy with training dataset: 81.08666666666666%\n",
      "Accuracy with test dataset: 80.99166666666666%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: CrossEntropyLoss, Activation: relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.5154519879817963\n",
      "Epoch 1, Batch 200, Loss: 0.8045888113975525\n",
      "Epoch 1, Batch 300, Loss: 0.6483622096478939\n",
      "Epoch 1, Batch 400, Loss: 0.5398481696844101\n",
      "Epoch 1, Batch 500, Loss: 0.4830939035117626\n",
      "Epoch 1, Batch 600, Loss: 0.47483624905347827\n",
      "Epoch 1, Batch 700, Loss: 0.4472166341543198\n",
      "Epoch 1, Batch 800, Loss: 0.39697365686297414\n",
      "Epoch 1, Batch 900, Loss: 0.3890017193555832\n",
      "Epoch 2, Batch 100, Loss: 0.3524292860925198\n",
      "Epoch 2, Batch 200, Loss: 0.30817696183919907\n",
      "Epoch 2, Batch 300, Loss: 0.30336361192166805\n",
      "Epoch 2, Batch 400, Loss: 0.3096317158639431\n",
      "Epoch 2, Batch 500, Loss: 0.30622368708252906\n",
      "Epoch 2, Batch 600, Loss: 0.2908535538613796\n",
      "Epoch 2, Batch 700, Loss: 0.2850087809562683\n",
      "Epoch 2, Batch 800, Loss: 0.276300103738904\n",
      "Epoch 2, Batch 900, Loss: 0.2852535865455866\n",
      "Epoch 3, Batch 100, Loss: 0.25111525487154723\n",
      "Epoch 3, Batch 200, Loss: 0.23624702006578446\n",
      "Epoch 3, Batch 300, Loss: 0.25877672992646694\n",
      "Epoch 3, Batch 400, Loss: 0.24392634194344281\n",
      "Epoch 3, Batch 500, Loss: 0.21841309547424317\n",
      "Epoch 3, Batch 600, Loss: 0.2367203065007925\n",
      "Epoch 3, Batch 700, Loss: 0.2403875045478344\n",
      "Epoch 3, Batch 800, Loss: 0.21211195401847363\n",
      "Epoch 3, Batch 900, Loss: 0.22292193420231343\n",
      "Epoch 4, Batch 100, Loss: 0.20845801059156657\n",
      "Epoch 4, Batch 200, Loss: 0.21660106938332319\n",
      "Epoch 4, Batch 300, Loss: 0.19712699938565492\n",
      "Epoch 4, Batch 400, Loss: 0.1883524375036359\n",
      "Epoch 4, Batch 500, Loss: 0.20357916478067636\n",
      "Epoch 4, Batch 600, Loss: 0.183854847997427\n",
      "Epoch 4, Batch 700, Loss: 0.19751928880810737\n",
      "Epoch 4, Batch 800, Loss: 0.19198173083364964\n",
      "Epoch 4, Batch 900, Loss: 0.2090895741805434\n",
      "Epoch 5, Batch 100, Loss: 0.16977361902594568\n",
      "Epoch 5, Batch 200, Loss: 0.17590358272194861\n",
      "Epoch 5, Batch 300, Loss: 0.18979157239198685\n",
      "Epoch 5, Batch 400, Loss: 0.18533543676137923\n",
      "Epoch 5, Batch 500, Loss: 0.17815454594790936\n",
      "Epoch 5, Batch 600, Loss: 0.18134509541094304\n",
      "Epoch 5, Batch 700, Loss: 0.15581811554729938\n",
      "Epoch 5, Batch 800, Loss: 0.18100167086347937\n",
      "Epoch 5, Batch 900, Loss: 0.16364388592541218\n",
      "Finished Training\n",
      "Accuracy with training dataset: 96.91666666666666%\n",
      "Accuracy with test dataset: 95.93333333333334%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: CrossEntropyLoss, Activation: leaky_relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.413167925477028\n",
      "Epoch 1, Batch 200, Loss: 0.7297158163785934\n",
      "Epoch 1, Batch 300, Loss: 0.589624920785427\n",
      "Epoch 1, Batch 400, Loss: 0.5320206207036972\n",
      "Epoch 1, Batch 500, Loss: 0.49034632608294487\n",
      "Epoch 1, Batch 600, Loss: 0.4678806510567665\n",
      "Epoch 1, Batch 700, Loss: 0.42925854310393335\n",
      "Epoch 1, Batch 800, Loss: 0.3860296878218651\n",
      "Epoch 1, Batch 900, Loss: 0.3732364517450333\n",
      "Epoch 2, Batch 100, Loss: 0.32560729429125784\n",
      "Epoch 2, Batch 200, Loss: 0.3184389565885067\n",
      "Epoch 2, Batch 300, Loss: 0.31367254376411435\n",
      "Epoch 2, Batch 400, Loss: 0.28698351208120587\n",
      "Epoch 2, Batch 500, Loss: 0.29377363495528697\n",
      "Epoch 2, Batch 600, Loss: 0.28392812356352803\n",
      "Epoch 2, Batch 700, Loss: 0.2675262321531773\n",
      "Epoch 2, Batch 800, Loss: 0.2560728576779365\n",
      "Epoch 2, Batch 900, Loss: 0.24031324982643126\n",
      "Epoch 3, Batch 100, Loss: 0.21750390253961085\n",
      "Epoch 3, Batch 200, Loss: 0.22000279009342194\n",
      "Epoch 3, Batch 300, Loss: 0.2205043735355139\n",
      "Epoch 3, Batch 400, Loss: 0.22979895353317262\n",
      "Epoch 3, Batch 500, Loss: 0.20640562824904918\n",
      "Epoch 3, Batch 600, Loss: 0.20734623290598392\n",
      "Epoch 3, Batch 700, Loss: 0.19926271446049212\n",
      "Epoch 3, Batch 800, Loss: 0.19885774537920953\n",
      "Epoch 3, Batch 900, Loss: 0.21413013372570278\n",
      "Epoch 4, Batch 100, Loss: 0.20518346477299929\n",
      "Epoch 4, Batch 200, Loss: 0.1804735865816474\n",
      "Epoch 4, Batch 300, Loss: 0.18106330301612616\n",
      "Epoch 4, Batch 400, Loss: 0.17962636336684226\n",
      "Epoch 4, Batch 500, Loss: 0.18193463556468487\n",
      "Epoch 4, Batch 600, Loss: 0.16024549387395381\n",
      "Epoch 4, Batch 700, Loss: 0.1742870758846402\n",
      "Epoch 4, Batch 800, Loss: 0.1618467822484672\n",
      "Epoch 4, Batch 900, Loss: 0.16982292221859097\n",
      "Epoch 5, Batch 100, Loss: 0.1594428613781929\n",
      "Epoch 5, Batch 200, Loss: 0.17479884818196298\n",
      "Epoch 5, Batch 300, Loss: 0.16771188773214818\n",
      "Epoch 5, Batch 400, Loss: 0.1624030045233667\n",
      "Epoch 5, Batch 500, Loss: 0.1577225737646222\n",
      "Epoch 5, Batch 600, Loss: 0.1559865265339613\n",
      "Epoch 5, Batch 700, Loss: 0.14538553416728972\n",
      "Epoch 5, Batch 800, Loss: 0.16332593265920878\n",
      "Epoch 5, Batch 900, Loss: 0.15346769321709872\n",
      "Finished Training\n",
      "Accuracy with training dataset: 97.33166666666668%\n",
      "Accuracy with test dataset: 96.48666666666666%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: CrossEntropyLoss, Activation: elu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.2924477791786193\n",
      "Epoch 1, Batch 200, Loss: 0.683157739341259\n",
      "Epoch 1, Batch 300, Loss: 0.5356015077233315\n",
      "Epoch 1, Batch 400, Loss: 0.4904741103947163\n",
      "Epoch 1, Batch 500, Loss: 0.4288084836304188\n",
      "Epoch 1, Batch 600, Loss: 0.42315691217780116\n",
      "Epoch 1, Batch 700, Loss: 0.38252644285559656\n",
      "Epoch 1, Batch 800, Loss: 0.37997195675969125\n",
      "Epoch 1, Batch 900, Loss: 0.34821158975362776\n",
      "Epoch 2, Batch 100, Loss: 0.3426344432681799\n",
      "Epoch 2, Batch 200, Loss: 0.29964291825890543\n",
      "Epoch 2, Batch 300, Loss: 0.28012139573693273\n",
      "Epoch 2, Batch 400, Loss: 0.28822083309292795\n",
      "Epoch 2, Batch 500, Loss: 0.2773182568699121\n",
      "Epoch 2, Batch 600, Loss: 0.2550398241728544\n",
      "Epoch 2, Batch 700, Loss: 0.2640297901630402\n",
      "Epoch 2, Batch 800, Loss: 0.25045464783906934\n",
      "Epoch 2, Batch 900, Loss: 0.2421645648404956\n",
      "Epoch 3, Batch 100, Loss: 0.2053152739442885\n",
      "Epoch 3, Batch 200, Loss: 0.21695536233484744\n",
      "Epoch 3, Batch 300, Loss: 0.2010197563096881\n",
      "Epoch 3, Batch 400, Loss: 0.2297452187538147\n",
      "Epoch 3, Batch 500, Loss: 0.20867343116551637\n",
      "Epoch 3, Batch 600, Loss: 0.20625157425180077\n",
      "Epoch 3, Batch 700, Loss: 0.23197039410471917\n",
      "Epoch 3, Batch 800, Loss: 0.18996475096791982\n",
      "Epoch 3, Batch 900, Loss: 0.18205449499189855\n",
      "Epoch 4, Batch 100, Loss: 0.17728150572627782\n",
      "Epoch 4, Batch 200, Loss: 0.16901084396988153\n",
      "Epoch 4, Batch 300, Loss: 0.17486851012334229\n",
      "Epoch 4, Batch 400, Loss: 0.17889555849134922\n",
      "Epoch 4, Batch 500, Loss: 0.16932696264237165\n",
      "Epoch 4, Batch 600, Loss: 0.18131367748603225\n",
      "Epoch 4, Batch 700, Loss: 0.1728223953396082\n",
      "Epoch 4, Batch 800, Loss: 0.17262114085257052\n",
      "Epoch 4, Batch 900, Loss: 0.17143398884683847\n",
      "Epoch 5, Batch 100, Loss: 0.16317648638039828\n",
      "Epoch 5, Batch 200, Loss: 0.14276014825329184\n",
      "Epoch 5, Batch 300, Loss: 0.15180738631635904\n",
      "Epoch 5, Batch 400, Loss: 0.1632335463911295\n",
      "Epoch 5, Batch 500, Loss: 0.16068199045956136\n",
      "Epoch 5, Batch 600, Loss: 0.1622789119556546\n",
      "Epoch 5, Batch 700, Loss: 0.1485721524246037\n",
      "Epoch 5, Batch 800, Loss: 0.16906185690313577\n",
      "Epoch 5, Batch 900, Loss: 0.13578355187550187\n",
      "Finished Training\n",
      "Accuracy with training dataset: 96.96333333333334%\n",
      "Accuracy with test dataset: 96.11833333333333%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: NLLLoss, Activation: relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.3377479845285416\n",
      "Epoch 1, Batch 200, Loss: 0.6891999670863151\n",
      "Epoch 1, Batch 300, Loss: 0.5756697535514832\n",
      "Epoch 1, Batch 400, Loss: 0.5027597457170486\n",
      "Epoch 1, Batch 500, Loss: 0.46342584654688834\n",
      "Epoch 1, Batch 600, Loss: 0.4239462485909462\n",
      "Epoch 1, Batch 700, Loss: 0.41148681730031966\n",
      "Epoch 1, Batch 800, Loss: 0.39611837089061736\n",
      "Epoch 1, Batch 900, Loss: 0.36802070677280424\n",
      "Epoch 2, Batch 100, Loss: 0.31919029787182807\n",
      "Epoch 2, Batch 200, Loss: 0.30797628238797187\n",
      "Epoch 2, Batch 300, Loss: 0.3183025600761175\n",
      "Epoch 2, Batch 400, Loss: 0.29813095591962335\n",
      "Epoch 2, Batch 500, Loss: 0.2834081830084324\n",
      "Epoch 2, Batch 600, Loss: 0.2791259998828173\n",
      "Epoch 2, Batch 700, Loss: 0.268496140986681\n",
      "Epoch 2, Batch 800, Loss: 0.24399352058768273\n",
      "Epoch 2, Batch 900, Loss: 0.25676156230270863\n",
      "Epoch 3, Batch 100, Loss: 0.22806608472019435\n",
      "Epoch 3, Batch 200, Loss: 0.23344239011406898\n",
      "Epoch 3, Batch 300, Loss: 0.23906579926609994\n",
      "Epoch 3, Batch 400, Loss: 0.22094177249819041\n",
      "Epoch 3, Batch 500, Loss: 0.21035372626036405\n",
      "Epoch 3, Batch 600, Loss: 0.20926657907664775\n",
      "Epoch 3, Batch 700, Loss: 0.21468132466077805\n",
      "Epoch 3, Batch 800, Loss: 0.21869904758408665\n",
      "Epoch 3, Batch 900, Loss: 0.21127949766814708\n",
      "Epoch 4, Batch 100, Loss: 0.18238312121480704\n",
      "Epoch 4, Batch 200, Loss: 0.18890162959694862\n",
      "Epoch 4, Batch 300, Loss: 0.1979350145533681\n",
      "Epoch 4, Batch 400, Loss: 0.18853865280747414\n",
      "Epoch 4, Batch 500, Loss: 0.2018494411557913\n",
      "Epoch 4, Batch 600, Loss: 0.1666841272637248\n",
      "Epoch 4, Batch 700, Loss: 0.1675768254697323\n",
      "Epoch 4, Batch 800, Loss: 0.1794720359146595\n",
      "Epoch 4, Batch 900, Loss: 0.17379783410578967\n",
      "Epoch 5, Batch 100, Loss: 0.17780126933008433\n",
      "Epoch 5, Batch 200, Loss: 0.1583642940968275\n",
      "Epoch 5, Batch 300, Loss: 0.16426808275282384\n",
      "Epoch 5, Batch 400, Loss: 0.1715331656113267\n",
      "Epoch 5, Batch 500, Loss: 0.15844558518379925\n",
      "Epoch 5, Batch 600, Loss: 0.1524516406096518\n",
      "Epoch 5, Batch 700, Loss: 0.1696985966898501\n",
      "Epoch 5, Batch 800, Loss: 0.1567849662899971\n",
      "Epoch 5, Batch 900, Loss: 0.16101838652044534\n",
      "Finished Training\n",
      "Accuracy with training dataset: 97.18%\n",
      "Accuracy with test dataset: 96.17333333333333%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: NLLLoss, Activation: leaky_relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.4869846349954605\n",
      "Epoch 1, Batch 200, Loss: 0.7546868085861206\n",
      "Epoch 1, Batch 300, Loss: 0.5661710830032826\n",
      "Epoch 1, Batch 400, Loss: 0.5122683508694172\n",
      "Epoch 1, Batch 500, Loss: 0.44034686133265494\n",
      "Epoch 1, Batch 600, Loss: 0.4444205817580223\n",
      "Epoch 1, Batch 700, Loss: 0.3836482124775648\n",
      "Epoch 1, Batch 800, Loss: 0.36296137392520905\n",
      "Epoch 1, Batch 900, Loss: 0.3456690980494022\n",
      "Epoch 2, Batch 100, Loss: 0.32573785066604616\n",
      "Epoch 2, Batch 200, Loss: 0.30238836295902727\n",
      "Epoch 2, Batch 300, Loss: 0.2996242746710777\n",
      "Epoch 2, Batch 400, Loss: 0.2809439688920975\n",
      "Epoch 2, Batch 500, Loss: 0.27963134154677394\n",
      "Epoch 2, Batch 600, Loss: 0.2599600997567177\n",
      "Epoch 2, Batch 700, Loss: 0.26506477274000645\n",
      "Epoch 2, Batch 800, Loss: 0.2601117712631822\n",
      "Epoch 2, Batch 900, Loss: 0.246408873796463\n",
      "Epoch 3, Batch 100, Loss: 0.22630101222544907\n",
      "Epoch 3, Batch 200, Loss: 0.2357506290823221\n",
      "Epoch 3, Batch 300, Loss: 0.21462412297725678\n",
      "Epoch 3, Batch 400, Loss: 0.23164176408201456\n",
      "Epoch 3, Batch 500, Loss: 0.2216168598085642\n",
      "Epoch 3, Batch 600, Loss: 0.19605568934231996\n",
      "Epoch 3, Batch 700, Loss: 0.2336345562338829\n",
      "Epoch 3, Batch 800, Loss: 0.20463409120216966\n",
      "Epoch 3, Batch 900, Loss: 0.21728070341050626\n",
      "Epoch 4, Batch 100, Loss: 0.1850710916891694\n",
      "Epoch 4, Batch 200, Loss: 0.18026295773684978\n",
      "Epoch 4, Batch 300, Loss: 0.18323580918833612\n",
      "Epoch 4, Batch 400, Loss: 0.171495708078146\n",
      "Epoch 4, Batch 500, Loss: 0.17700799562036992\n",
      "Epoch 4, Batch 600, Loss: 0.19987371530383824\n",
      "Epoch 4, Batch 700, Loss: 0.18621011454612016\n",
      "Epoch 4, Batch 800, Loss: 0.1773910128325224\n",
      "Epoch 4, Batch 900, Loss: 0.19043156199157238\n",
      "Epoch 5, Batch 100, Loss: 0.15921010596677662\n",
      "Epoch 5, Batch 200, Loss: 0.17081026365980506\n",
      "Epoch 5, Batch 300, Loss: 0.16439650997519492\n",
      "Epoch 5, Batch 400, Loss: 0.17087293596938252\n",
      "Epoch 5, Batch 500, Loss: 0.15669244445860386\n",
      "Epoch 5, Batch 600, Loss: 0.16257095113396644\n",
      "Epoch 5, Batch 700, Loss: 0.18129173405468463\n",
      "Epoch 5, Batch 800, Loss: 0.1656645416468382\n",
      "Epoch 5, Batch 900, Loss: 0.15919872187078\n",
      "Finished Training\n",
      "Accuracy with training dataset: 96.905%\n",
      "Accuracy with test dataset: 96.10833333333333%\n",
      "\n",
      "Experimenting with Optimizer: Adam, Loss: NLLLoss, Activation: elu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.3776174211502075\n",
      "Epoch 1, Batch 200, Loss: 0.6975474986433983\n",
      "Epoch 1, Batch 300, Loss: 0.5801286378502846\n",
      "Epoch 1, Batch 400, Loss: 0.5185178074240685\n",
      "Epoch 1, Batch 500, Loss: 0.4630126817524433\n",
      "Epoch 1, Batch 600, Loss: 0.42492411777377126\n",
      "Epoch 1, Batch 700, Loss: 0.40924831047654153\n",
      "Epoch 1, Batch 800, Loss: 0.38732663333415984\n",
      "Epoch 1, Batch 900, Loss: 0.3515747357904911\n",
      "Epoch 2, Batch 100, Loss: 0.34786559000611306\n",
      "Epoch 2, Batch 200, Loss: 0.3317027409374714\n",
      "Epoch 2, Batch 300, Loss: 0.29159846253693106\n",
      "Epoch 2, Batch 400, Loss: 0.2823196066915989\n",
      "Epoch 2, Batch 500, Loss: 0.28896821051836014\n",
      "Epoch 2, Batch 600, Loss: 0.2589369594305754\n",
      "Epoch 2, Batch 700, Loss: 0.25888426959514615\n",
      "Epoch 2, Batch 800, Loss: 0.2657381153851748\n",
      "Epoch 2, Batch 900, Loss: 0.26331641383469107\n",
      "Epoch 3, Batch 100, Loss: 0.22781397845596074\n",
      "Epoch 3, Batch 200, Loss: 0.23636813376098872\n",
      "Epoch 3, Batch 300, Loss: 0.21757046401500701\n",
      "Epoch 3, Batch 400, Loss: 0.2164068742841482\n",
      "Epoch 3, Batch 500, Loss: 0.21492356594651937\n",
      "Epoch 3, Batch 600, Loss: 0.21995617739856244\n",
      "Epoch 3, Batch 700, Loss: 0.19469883188605308\n",
      "Epoch 3, Batch 800, Loss: 0.19257370691746473\n",
      "Epoch 3, Batch 900, Loss: 0.2044632114097476\n",
      "Epoch 4, Batch 100, Loss: 0.181136415861547\n",
      "Epoch 4, Batch 200, Loss: 0.17853989005088805\n",
      "Epoch 4, Batch 300, Loss: 0.1790857283398509\n",
      "Epoch 4, Batch 400, Loss: 0.1914649135619402\n",
      "Epoch 4, Batch 500, Loss: 0.17780990205705166\n",
      "Epoch 4, Batch 600, Loss: 0.19215882278978824\n",
      "Epoch 4, Batch 700, Loss: 0.18609428089112043\n",
      "Epoch 4, Batch 800, Loss: 0.16456254106014967\n",
      "Epoch 4, Batch 900, Loss: 0.1607233590260148\n",
      "Epoch 5, Batch 100, Loss: 0.15585053618997335\n",
      "Epoch 5, Batch 200, Loss: 0.1688824111968279\n",
      "Epoch 5, Batch 300, Loss: 0.1775150326639414\n",
      "Epoch 5, Batch 400, Loss: 0.1503658324852586\n",
      "Epoch 5, Batch 500, Loss: 0.18230599142611026\n",
      "Epoch 5, Batch 600, Loss: 0.15140912249684335\n",
      "Epoch 5, Batch 700, Loss: 0.15997646789997816\n",
      "Epoch 5, Batch 800, Loss: 0.1578682930395007\n",
      "Epoch 5, Batch 900, Loss: 0.16026423133909704\n",
      "Finished Training\n",
      "Accuracy with training dataset: 97.0%\n",
      "Accuracy with test dataset: 96.14666666666668%\n",
      "\n",
      "Experimenting with Optimizer: RMSprop, Loss: CrossEntropyLoss, Activation: relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.3239566475152968\n",
      "Epoch 1, Batch 200, Loss: 0.6751082617044449\n",
      "Epoch 1, Batch 300, Loss: 0.5791442485153675\n",
      "Epoch 1, Batch 400, Loss: 0.5217033627629281\n",
      "Epoch 1, Batch 500, Loss: 0.4352021895349026\n",
      "Epoch 1, Batch 600, Loss: 0.4268589524924755\n",
      "Epoch 1, Batch 700, Loss: 0.37513405591249466\n",
      "Epoch 1, Batch 800, Loss: 0.357983335852623\n",
      "Epoch 1, Batch 900, Loss: 0.3421951587498188\n",
      "Epoch 2, Batch 100, Loss: 0.32481715686619284\n",
      "Epoch 2, Batch 200, Loss: 0.30853996977210046\n",
      "Epoch 2, Batch 300, Loss: 0.3159169950336218\n",
      "Epoch 2, Batch 400, Loss: 0.27762683868408206\n",
      "Epoch 2, Batch 500, Loss: 0.262901526838541\n",
      "Epoch 2, Batch 600, Loss: 0.27503448486328125\n",
      "Epoch 2, Batch 700, Loss: 0.2529950477182865\n",
      "Epoch 2, Batch 800, Loss: 0.24794471882283686\n",
      "Epoch 2, Batch 900, Loss: 0.24106532089412214\n",
      "Epoch 3, Batch 100, Loss: 0.22752415850758553\n",
      "Epoch 3, Batch 200, Loss: 0.2153210887312889\n",
      "Epoch 3, Batch 300, Loss: 0.2301579674705863\n",
      "Epoch 3, Batch 400, Loss: 0.2054871967434883\n",
      "Epoch 3, Batch 500, Loss: 0.2232374918460846\n",
      "Epoch 3, Batch 600, Loss: 0.2219351054355502\n",
      "Epoch 3, Batch 700, Loss: 0.19624357510358095\n",
      "Epoch 3, Batch 800, Loss: 0.2164233277924359\n",
      "Epoch 3, Batch 900, Loss: 0.20965248629450797\n",
      "Epoch 4, Batch 100, Loss: 0.1919303724169731\n",
      "Epoch 4, Batch 200, Loss: 0.19260256487876176\n",
      "Epoch 4, Batch 300, Loss: 0.1849257719144225\n",
      "Epoch 4, Batch 400, Loss: 0.18702906653285026\n",
      "Epoch 4, Batch 500, Loss: 0.17743363922461866\n",
      "Epoch 4, Batch 600, Loss: 0.1755390220694244\n",
      "Epoch 4, Batch 700, Loss: 0.1932685348391533\n",
      "Epoch 4, Batch 800, Loss: 0.19722709119319914\n",
      "Epoch 4, Batch 900, Loss: 0.19589721534401178\n",
      "Epoch 5, Batch 100, Loss: 0.15394824082031847\n",
      "Epoch 5, Batch 200, Loss: 0.18424380496144294\n",
      "Epoch 5, Batch 300, Loss: 0.17217488875612616\n",
      "Epoch 5, Batch 400, Loss: 0.1466649334691465\n",
      "Epoch 5, Batch 500, Loss: 0.18064158506691455\n",
      "Epoch 5, Batch 600, Loss: 0.165217822548002\n",
      "Epoch 5, Batch 700, Loss: 0.1843971951305866\n",
      "Epoch 5, Batch 800, Loss: 0.1813642719388008\n",
      "Epoch 5, Batch 900, Loss: 0.15968874042853712\n",
      "Finished Training\n",
      "Accuracy with training dataset: 96.88833333333334%\n",
      "Accuracy with test dataset: 95.96666666666667%\n",
      "\n",
      "Experimenting with Optimizer: RMSprop, Loss: CrossEntropyLoss, Activation: leaky_relu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.2274313402175903\n",
      "Epoch 1, Batch 200, Loss: 0.6413937509059906\n",
      "Epoch 1, Batch 300, Loss: 0.5340128982067108\n",
      "Epoch 1, Batch 400, Loss: 0.45697664499282836\n",
      "Epoch 1, Batch 500, Loss: 0.4158958813548088\n",
      "Epoch 1, Batch 600, Loss: 0.3833188419789076\n",
      "Epoch 1, Batch 700, Loss: 0.36033802330493925\n",
      "Epoch 1, Batch 800, Loss: 0.344312569051981\n",
      "Epoch 1, Batch 900, Loss: 0.33437473595142364\n",
      "Epoch 2, Batch 100, Loss: 0.30023478507995605\n",
      "Epoch 2, Batch 200, Loss: 0.28407901868224145\n",
      "Epoch 2, Batch 300, Loss: 0.27615675032138826\n",
      "Epoch 2, Batch 400, Loss: 0.2660205102711916\n",
      "Epoch 2, Batch 500, Loss: 0.27772957339882853\n",
      "Epoch 2, Batch 600, Loss: 0.254836703017354\n",
      "Epoch 2, Batch 700, Loss: 0.2554936896264553\n",
      "Epoch 2, Batch 800, Loss: 0.2541242542117834\n",
      "Epoch 2, Batch 900, Loss: 0.2358690869808197\n",
      "Epoch 3, Batch 100, Loss: 0.2132276164740324\n",
      "Epoch 3, Batch 200, Loss: 0.2247080799564719\n",
      "Epoch 3, Batch 300, Loss: 0.22182191528379916\n",
      "Epoch 3, Batch 400, Loss: 0.2037366675771773\n",
      "Epoch 3, Batch 500, Loss: 0.2095629756525159\n",
      "Epoch 3, Batch 600, Loss: 0.2135125505924225\n",
      "Epoch 3, Batch 700, Loss: 0.21502953298389912\n",
      "Epoch 3, Batch 800, Loss: 0.2009417501091957\n",
      "Epoch 3, Batch 900, Loss: 0.19306448470801116\n",
      "Epoch 4, Batch 100, Loss: 0.17274384517222643\n",
      "Epoch 4, Batch 200, Loss: 0.19394067734479903\n",
      "Epoch 4, Batch 300, Loss: 0.18711364652961493\n",
      "Epoch 4, Batch 400, Loss: 0.1843979226052761\n",
      "Epoch 4, Batch 500, Loss: 0.17506057828664778\n",
      "Epoch 4, Batch 600, Loss: 0.17070414524525404\n",
      "Epoch 4, Batch 700, Loss: 0.18707269128412007\n",
      "Epoch 4, Batch 800, Loss: 0.18252095755189657\n",
      "Epoch 4, Batch 900, Loss: 0.19035051263868807\n",
      "Epoch 5, Batch 100, Loss: 0.1786796118877828\n",
      "Epoch 5, Batch 200, Loss: 0.16358240880072117\n",
      "Epoch 5, Batch 300, Loss: 0.16992301670834423\n",
      "Epoch 5, Batch 400, Loss: 0.16310122195631266\n",
      "Epoch 5, Batch 500, Loss: 0.17082760572433472\n",
      "Epoch 5, Batch 600, Loss: 0.16589744195342063\n",
      "Epoch 5, Batch 700, Loss: 0.15777354180812836\n",
      "Epoch 5, Batch 800, Loss: 0.15336411060765387\n",
      "Epoch 5, Batch 900, Loss: 0.15247001698240636\n",
      "Finished Training\n",
      "Accuracy with training dataset: 95.525%\n",
      "Accuracy with test dataset: 94.74333333333334%\n",
      "\n",
      "Experimenting with Optimizer: RMSprop, Loss: CrossEntropyLoss, Activation: elu\n",
      "\n",
      "Epoch 1, Batch 100, Loss: 1.3876072019338608\n",
      "Epoch 1, Batch 200, Loss: 0.7479615151882172\n",
      "Epoch 1, Batch 300, Loss: 0.6154910948872566\n",
      "Epoch 1, Batch 400, Loss: 0.5377989542484284\n",
      "Epoch 1, Batch 500, Loss: 0.49787673622369766\n",
      "Epoch 1, Batch 600, Loss: 0.4315132954716682\n",
      "Epoch 1, Batch 700, Loss: 0.4234551151096821\n",
      "Epoch 1, Batch 800, Loss: 0.4163799560070038\n",
      "Epoch 1, Batch 900, Loss: 0.3681290492415428\n",
      "Epoch 2, Batch 100, Loss: 0.3230757150799036\n",
      "Epoch 2, Batch 200, Loss: 0.3450802632421255\n",
      "Epoch 2, Batch 300, Loss: 0.3226263197511435\n",
      "Epoch 2, Batch 400, Loss: 0.32209264516830444\n",
      "Epoch 2, Batch 500, Loss: 0.2931224650889635\n",
      "Epoch 2, Batch 600, Loss: 0.2935048781335354\n",
      "Epoch 2, Batch 700, Loss: 0.29691985219717026\n",
      "Epoch 2, Batch 800, Loss: 0.2580631685256958\n",
      "Epoch 2, Batch 900, Loss: 0.2660219592601061\n",
      "Epoch 3, Batch 100, Loss: 0.23972223602235318\n",
      "Epoch 3, Batch 200, Loss: 0.2580569909512997\n",
      "Epoch 3, Batch 300, Loss: 0.24197018921375274\n",
      "Epoch 3, Batch 400, Loss: 0.21577246934175492\n",
      "Epoch 3, Batch 500, Loss: 0.23093491077423095\n",
      "Epoch 3, Batch 600, Loss: 0.23992980495095254\n",
      "Epoch 3, Batch 700, Loss: 0.22333392925560475\n",
      "Epoch 3, Batch 800, Loss: 0.2344935653358698\n",
      "Epoch 3, Batch 900, Loss: 0.21719843585044146\n",
      "Epoch 4, Batch 100, Loss: 0.2143656602501869\n",
      "Epoch 4, Batch 200, Loss: 0.20142821855843068\n",
      "Epoch 4, Batch 300, Loss: 0.17885481890290975\n",
      "Epoch 4, Batch 400, Loss: 0.20639605216681958\n",
      "Epoch 4, Batch 500, Loss: 0.20175495862960816\n",
      "Epoch 4, Batch 600, Loss: 0.21307413622736932\n",
      "Epoch 4, Batch 700, Loss: 0.17540609441697597\n",
      "Epoch 4, Batch 800, Loss: 0.2012272247672081\n",
      "Epoch 4, Batch 900, Loss: 0.19425842311233282\n",
      "Epoch 5, Batch 100, Loss: 0.18419951904565096\n",
      "Epoch 5, Batch 200, Loss: 0.181201843470335\n",
      "Epoch 5, Batch 300, Loss: 0.16554296031594276\n",
      "Epoch 5, Batch 400, Loss: 0.1720243287831545\n",
      "Epoch 5, Batch 500, Loss: 0.18839226603507997\n",
      "Epoch 5, Batch 600, Loss: 0.17266996592283249\n",
      "Epoch 5, Batch 700, Loss: 0.17468385487794877\n",
      "Epoch 5, Batch 800, Loss: 0.1710081499814987\n",
      "Epoch 5, Batch 900, Loss: 0.18417942782863975\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# List of optimizers, loss functions, and activation functions\n",
    "optimizers = [optim.SGD, optim.Adam, optim.RMSprop]\n",
    "loss_functions = [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "activation_functions = [F.relu, F.leaky_relu, F.elu]\n",
    "\n",
    "# Store results in a list\n",
    "results = []\n",
    "\n",
    "# Experiment with different combinations\n",
    "for opt_class in optimizers:\n",
    "    for loss_class in loss_functions:\n",
    "        for activation_func in activation_functions:\n",
    "            model = TunedMLP(dropout_rate=dropout_rate)\n",
    "            optimizer = opt_class(model.parameters(), lr=learning_rate)\n",
    "            criterion = loss_class()\n",
    "            \n",
    "            # Set the activation function in the model\n",
    "            model.fc1.activation = activation_func\n",
    "            model.fc2.activation = activation_func\n",
    "\n",
    "            print(f\"\\nExperimenting with Optimizer: {opt_class.__name__}, Loss: {loss_class.__name__}, Activation: {activation_func.__name__}\\n\")\n",
    "            \n",
    "            training(model, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "            # Evaluate on training set\n",
    "            train_accuracy = evalModelwithTrain(model, train_loader)\n",
    "\n",
    "            # Evaluate on test set (assuming evalModelwithTest returns accuracy)\n",
    "            test_accuracy = evalModelwithTest(model, test_loader)\n",
    "\n",
    "            # Store the results\n",
    "            result_entry = {\n",
    "                'Optimizer': opt_class.__name__,\n",
    "                'Loss Function': loss_class.__name__,\n",
    "                'Activation Function': activation_func.__name__,\n",
    "                'Learning Rate': learning_rate,\n",
    "                'Dropout Rate': dropout_rate,\n",
    "                'Train Accuracy': train_accuracy,\n",
    "                'Test Accuracy': test_accuracy\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c691e-afbe-44db-ae7a-f199e685c7ae",
   "metadata": {},
   "source": [
    "# Observations from the results of the models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8b97e-d943-43f7-91dd-facb915ccac7",
   "metadata": {},
   "source": [
    "1. The model trained with the Adam optimizer outperforms the one trained with SGD across different activation functions and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea41ed-fbea-4b3e-bd22-aa7af2d8b293",
   "metadata": {},
   "source": [
    "2. Among the tested activation functions (ReLU, Leaky ReLU, ELU), ReLU tends to perform the best, achieving the highest accuracy with both optimizers and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6524ab-26ab-45e2-8083-ee606100b891",
   "metadata": {},
   "source": [
    "3. Adam achieves higher accuracy on both training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bbd6c-55c0-42ec-873f-4e906d36e6b1",
   "metadata": {},
   "source": [
    "4. CrossEntropyLoss outperforms NLLLoss, achieving higher accuracies on both training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a879d-778b-43b3-aae7-6a02858db313",
   "metadata": {},
   "source": [
    "5. Across different experiments, the training loss consistently decreases over epochs, indicating that the models are learning and adapting to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db947202-87a1-41bf-a414-695b9275b391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e521888-056a-4aaa-98de-2655a581c620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
